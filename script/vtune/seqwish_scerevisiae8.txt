Elapsed Time: 23.314s
    IPC: 0.708
     | The IPC may be too low. This could be caused by issues such as memory
     | stalls, instruction starvation, branch misprediction or long latency
     | instructions. Explore the other hardware-related metrics to identify what
     | is causing low IPC.
     |
    DP GFLOPS: 0.000
    x87 GFLOPS: 0.002
    Average CPU Frequency: 3.202 GHz
Logical Core Utilization: 25.5% (8.167 out of 32)
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization. Consider improving physical core utilization as the first step
 | and then look at opportunities to utilize logical cores, which in some cases
 | can improve processor throughput and overall performance of multi-threaded
 | applications.
 |
    Physical Core Utilization: 25.5% (8.158 out of 32)
     | The metric value is low, which may signal a poor physical CPU cores
     | utilization caused by:
     |     - load imbalance
     |     - threading runtime overhead
     |     - contended synchronization
     |     - thread/process underutilization
     |     - incorrect affinity that utilizes logical cores instead of physical
     |       cores
     | Run the HPC Performance Characterization analysis to estimate the
     | efficiency of MPI and OpenMP parallelism or run the Locks and Waits
     | analysis to identify parallel bottlenecks for other parallel runtimes.
     |
Microarchitecture Usage: 18.5% of Pipeline Slots
 | You code efficiency on this platform is too low.
 | 
 | Possible cause: memory stalls, instruction starvation, branch misprediction
 | or long latency instructions.
 | 
 | Next steps: Run Microarchitecture Exploration analysis to identify the cause
 | of the low microarchitecture usage efficiency.
 |
    Retiring: 18.5% of Pipeline Slots
    Front-End Bound: 18.8% of Pipeline Slots
     | Issue: A significant portion of Pipeline Slots is remaining empty due to
     | issues in the Front-End.
     | 
     | Tips:  Make sure the code working size is not too large, the code layout
     | does not require too many memory accesses per cycle to get enough
     | instructions for filling four pipeline slots, or check for microcode
     | assists.
     |
    Bad Speculation: 2.9% of Pipeline Slots
    Back-End Bound: 59.8% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 34.4% of Pipeline Slots
         | The metric value is high. This can indicate that the significant
         | fraction of execution pipeline slots could be stalled due to demand
         | memory load and stores. Use Memory Access analysis to have the metric
         | breakdown by memory hierarchy, memory bandwidth information,
         | correlation by memory objects.
         |
            L1 Bound: 22.4% of Clockticks
             | This metric shows how often machine was stalled without missing
             | the L1 data cache. The L1 cache typically has the shortest
             | latency. However, in certain cases like loads blocked on older
             | stores, a load might suffer a high latency even though it is
             | being satisfied by the L1. Note that this metric value may be
             | highlighted due to DTLB Overhead or Cycles of 1 Port Utilized
             | issues.
             |
                FB Full: 20.0% of Clockticks
            L2 Bound: 1.0% of Clockticks
            L3 Bound: 14.1% of Clockticks
             | This metric shows how often CPU was stalled on L3 cache, or
             | contended with a sibling Core. Avoiding cache misses (L2
             | misses/L3 hits) improves the latency and increases performance.
             |
                L3 Latency: 0.4% of Clockticks
                 | This metric shows a fraction of cycles with demand load
                 | accesses that hit the L3 cache under unloaded scenarios
                 | (possibly L3 latency limited). Avoiding private cache misses
                 | (i.e. L2 misses/L3 hits) will improve the latency, reduce
                 | contention with sibling physical cores and increase
                 | performance. Note the value of this node may overlap with its
                 | siblings.
                 |
            DRAM Bound: 6.8% of Clockticks
                Memory Bandwidth: 7.0% of Clockticks
                Memory Latency: 22.4% of Clockticks
                    Local DRAM: 0.9% of Clockticks
                    Remote DRAM: 0.6% of Clockticks
                    Remote Cache: 1.5% of Clockticks
            Store Bound: 2.5% of Clockticks
        Core Bound: 25.3% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
Memory Bound: 34.4% of Pipeline Slots
 | The metric value is high. This can indicate that the significant fraction of
 | execution pipeline slots could be stalled due to demand memory load and
 | stores. Use Memory Access analysis to have the metric breakdown by memory
 | hierarchy, memory bandwidth information, correlation by memory objects.
 |
    Cache Bound: 37.5% of Clockticks
     | A significant proportion of cycles are being spent on data fetches from
     | caches. Check Memory Access analysis to see if accesses to L2 or L3
     | caches are problematic and consider applying the same performance tuning
     | as you would for a cache-missing workload. This may include reducing the
     | data working set size, improving data access locality, blocking or
     | partitioning the working set to fit in the lower cache levels, or
     | exploiting hardware prefetchers. Consider using software prefetchers, but
     | note that they can interfere with normal loads, increase latency, and
     | increase pressure on the memory system. This metric includes coherence
     | penalties for shared data. Check Microarchitecture Exploration analysis
     | to see if contested accesses or data sharing are indicated as likely
     | issues.
     |
    DRAM Bound: 6.8% of Clockticks
        Average DRAM Bandwidth, GB/s: 0.000
    NUMA: % of Remote Accesses: 68.0%
Vectorization: 0.0% of Packed FP Operations
    Instruction Mix
        SP FLOPs: 0.0% of uOps
            Packed: 0.0% from SP FP
                128-bit: 0.0% from SP FP
                256-bit: 0.0% from SP FP
                512-bit: 0.0% from SP FP
            Scalar: 0.0% from SP FP
        DP FLOPs: 0.0% of uOps
            Packed: 0.0% from DP FP
                128-bit: 0.0% from DP FP
                256-bit: 0.0% from DP FP
                512-bit: 0.0% from DP FP
            Scalar: 100.0% from DP FP
        x87 FLOPs: 0.0% of uOps
        Non-FP: 100.0% of uOps
    FP Arith/Mem Rd Instr. Ratio: 0.000
    FP Arith/Mem Wr Instr. Ratio: 0.001
Collection and Platform Info
    Application Command Line: ./seqwish "--threads" "32" "--paf-alns=/home/bc526/characterize_pggb/dataset/scerevisiae8/scerevisiae8.fa.gz.paf" "--seqs=/home/bc526/characterize_pggb/dataset/scerevisiae8/scerevisiae8.fa.gz" "--gfa=DRB1-3123.fa.gz.scerevisiae8.gfa" "-k" "19" "-f" "0" "-B" "10000000" "-P" 
    Operating System: 3.10.0-1160.76.1.el7.x86_64 \S Kernel \r on an \m 
    Computer Name: zhang-24.ece.cornell.edu
    Result Size: 4.1 MB 
    Collection start time: 19:14:35 10/12/2023 UTC
    Collection stop time: 19:14:58 10/12/2023 UTC
    Collector Type: Driverless Perf per-process counting
    CPU
        Name: Intel(R) Xeon(R) Processor code named Cascadelake
        Frequency: 3.392 GHz
        Logical CPU Count: 32
        Max DRAM Single-Package Bandwidth: 103.000 GB/s
        LLC size: 37.5 MB 
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

Recommendations:
    Hotspots: Start with Hotspots analysis to understand the efficiency of your algorithm.
     | Use Hotspots analysis to identify the most time consuming functions.
     | Drill down to see the time spent on every line of code.
    Threading: There is poor utilization of logical CPU cores (25.5%) in your application.
     |  Use Threading to explore more opportunities to increase parallelism in
     | your application.
    Microarchitecture Exploration: There is low microarchitecture usage (18.5%) of available hardware resources.
     | Run Microarchitecture Exploration analysis to analyze CPU
     | microarchitecture bottlenecks that can affect application performance.
    Memory Access: The Memory Bound metric is high  (34.4%). A significant fraction of execution pipeline slots could be stalled due to demand memory load and stores.
     | Use Memory Access analysis to measure metrics that can identify memory
     | access issues.

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
